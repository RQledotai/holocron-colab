{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RQledotai/holocron-colab/blob/master/notebooks/whisper_to_synopsis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LNbtD84qH1a"
      },
      "source": [
        "# <center>\n",
        "<div align=\"center\">\n",
        "  <h1>Whisper to Synopsis\n",
        "  <br/>\n",
        "  <img src=\"https://raw.githubusercontent.com/RQledotai/holocron-colab/master/img/whisper-synopsis.png\" width=\"200\"/>\n",
        "  </h1>\n",
        "</div>\n",
        "</center>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Content\n",
        "- [Description](#description)\n",
        "- [Initialization](#initialization)\n",
        "  - [Installing Dependencies](#installing-dependencies)\n",
        "  - [Installing Vosk Model](#installing-vosk-model)\n",
        "- [Processing Video](#processing-video)\n",
        "  - [Downloading the YouTube Video](#downloading-video)\n",
        "  - [Extracting the Audio Track](#extracting-audio)\n",
        "  - [Transcribing the Audio](#transcribing-audio)\n",
        "  - [Summarizing the Transcribed Text](#summarizing-text)\n",
        "- [Conclusion](#conclusion)"
      ],
      "metadata": {
        "id": "8yr1BZvljGN0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWXMCRyPq34r"
      },
      "source": [
        "## Description <a name=\"description\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Sy0_vrqQwi"
      },
      "source": [
        "This Google Colab notebook demonstrates the end-to-end process of converting MP4 videos (downloaded from YouTube) into concise summaries using the power of [Google AI API](https://ai.google.dev/).\n",
        "\n",
        "The process involves the following steps:\n",
        "1. **Audio Extraction**: Isolating the audio track from the MP4 video.\n",
        "2. **Speech-to-Text Transcription**: Converting the extracting audio into text format.\n",
        "3. **Key Takeaway Summarization**: Leveraging Google AI API to analyze the transcribed text and generate a succinct summary of the video's key points.\n",
        "\n",
        "This offers a streamlined way to quickly grasp the essence of video content, saving users valuable time and effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-oOd1b9q9ui"
      },
      "source": [
        "## Initialization <a name=\"initialization\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before processing the video, we need to initialize runtime with the necessary Python libraries and artefacts."
      ],
      "metadata": {
        "id": "SqLwWWf8oBgd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WUqsBIarNMu"
      },
      "source": [
        "### Installing Dependencies <a name=\"installing-dependencies\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To download video from [YouTube](https://www.youtube.com/), we need to install the [`pytubefix` library](https://github.com/JuanBindez/pytubefix). The reason for selecting this libray is that it addresses a known issue with the standard `pytube` library (see [bug #1894](https://github.com/pytube/pytube/issues/1894#issue-2180600881))."
      ],
      "metadata": {
        "id": "_CMsYhjkokXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pytubefix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4md2aseVpwd9",
        "outputId": "f2549425-da09-44a2-b383-44753ab1cc52"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytubefix\n",
            "  Downloading pytubefix-6.10.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Downloading pytubefix-6.10.2-py3-none-any.whl (74 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/74.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.8/74.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytubefix\n",
            "Successfully installed pytubefix-6.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [`moviepy` library](https://zulko.github.io/moviepy/) is a Python module for video editing, which can be used for basic operations (like cuts, concatenations, title insertions), video compositing (a.k.a. non-linear editing), video processing, or to create advanced effects. In this notebook, the library will be used to extract the audio from the video."
      ],
      "metadata": {
        "id": "QkcH5D1Xp4zd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JuCYK8cpx2-",
        "outputId": "bc6f694a-a91b-4203-93fc-39dc6269bcaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.26.4)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.34.2)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.5.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (71.0.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "%pip install moviepy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To transcribe the audio into text, we will use the following libraries:\n",
        "* [`SpeechRecognition`](https://github.com/Uberi/speech_recognition): Interface to leverage different engines and APIs for performing speech recognition.\n",
        "* [`vosk-api`](): Toolkit to perform offline speech recognition"
      ],
      "metadata": {
        "id": "TKVonpB1qgPd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juqRT5-ntxrS",
        "outputId": "8d75f031-977c-43cc-e2e6-04db3a1fd566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting speechrecognition\n",
            "  Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl.metadata (28 kB)\n",
            "Collecting vosk\n",
            "  Downloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from speechrecognition) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from speechrecognition) (4.12.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from vosk) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from vosk) (4.66.5)\n",
            "Collecting srt (from vosk)\n",
            "  Downloading srt-3.5.3.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting websockets (from vosk)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->vosk) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->speechrecognition) (2024.7.4)\n",
            "Downloading SpeechRecognition-3.10.4-py2.py3-none-any.whl (32.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.8/32.8 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: srt\n",
            "  Building wheel for srt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for srt: filename=srt-3.5.3-py3-none-any.whl size=22428 sha256=03f4b1cd36e87dac84126aa50c0876397ec0f66b638ce922bca558caab8db341\n",
            "  Stored in directory: /root/.cache/pip/wheels/d7/31/a1/18e1e7e8bfdafd19e6803d7eb919b563dd11de380e4304e332\n",
            "Successfully built srt\n",
            "Installing collected packages: websockets, srt, vosk, speechrecognition\n",
            "Successfully installed speechrecognition-3.10.4 srt-3.5.3 vosk-0.3.45 websockets-12.0\n"
          ]
        }
      ],
      "source": [
        "%pip install speechrecognition vosk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: `vosk-api` requires a speech recognition to be available. The instructions to download / install a vosk model are available in the [*Installing Vosk Model*](#installing-vosk-model) section."
      ],
      "metadata": {
        "id": "LA4h1wsDrnWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [Google AI Python SDK](https://github.com/google-gemini/generative-ai-python) is the easiest way for Python developers to build with the Gemini API. Gemini models are built from the ground up to be multimodal, so you can reason seamlessly across text, images, and code."
      ],
      "metadata": {
        "id": "d3c-591SsTv9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaAiudGVuQjb",
        "outputId": "e7bcf879-ca2f-4c2e-c68e-aea58279201e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.20.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "%pip install google-generativeai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msTxL71e4G0L"
      },
      "source": [
        "### Installing Vosk Model <a name=\"installing-vosk-model\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4dMEv4R4SLz"
      },
      "source": [
        "As mentioned earlier, the `vosk-api` relies on speech recognition models to be available locally. There are many speech recognition models that can be leveraged (see [Model list](https://alphacephei.com/vosk/models)). For this notebook, we will leverage the small model for English (i.e. `vosk-model-small-en-us-0.15`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzX5lt1N-imd",
        "outputId": "bb4e2b6a-42dc-47da-d55e-a2a03ba0687d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-14 21:06:45--  https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
            "Resolving alphacephei.com (alphacephei.com)... 188.40.21.16, 2a01:4f8:13a:279f::2\n",
            "Connecting to alphacephei.com (alphacephei.com)|188.40.21.16|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41205931 (39M) [application/zip]\n",
            "Saving to: ‘vosk-model-small-en-us-0.15.zip’\n",
            "\n",
            "vosk-model-small-en 100%[===================>]  39.30M  12.7MB/s    in 3.1s    \n",
            "\n",
            "2024-08-14 21:06:49 (12.7 MB/s) - ‘vosk-model-small-en-us-0.15.zip’ saved [41205931/41205931]\n",
            "\n",
            "Archive:  vosk-model-small-en-us-0.15.zip\n",
            "   creating: vosk-model-small-en-us-0.15/\n",
            "   creating: vosk-model-small-en-us-0.15/am/\n",
            "  inflating: vosk-model-small-en-us-0.15/am/final.mdl  \n",
            "   creating: vosk-model-small-en-us-0.15/graph/\n",
            "  inflating: vosk-model-small-en-us-0.15/graph/disambig_tid.int  \n",
            "  inflating: vosk-model-small-en-us-0.15/graph/HCLr.fst  \n",
            "  inflating: vosk-model-small-en-us-0.15/graph/Gr.fst  \n",
            "   creating: vosk-model-small-en-us-0.15/graph/phones/\n",
            "  inflating: vosk-model-small-en-us-0.15/graph/phones/word_boundary.int  \n",
            "   creating: vosk-model-small-en-us-0.15/conf/\n",
            "  inflating: vosk-model-small-en-us-0.15/conf/model.conf  \n",
            "  inflating: vosk-model-small-en-us-0.15/conf/mfcc.conf  \n",
            "   creating: vosk-model-small-en-us-0.15/ivector/\n",
            "  inflating: vosk-model-small-en-us-0.15/ivector/splice.conf  \n",
            "  inflating: vosk-model-small-en-us-0.15/ivector/final.dubm  \n",
            "  inflating: vosk-model-small-en-us-0.15/ivector/global_cmvn.stats  \n",
            "  inflating: vosk-model-small-en-us-0.15/ivector/final.ie  \n",
            "  inflating: vosk-model-small-en-us-0.15/ivector/online_cmvn.conf  \n",
            "  inflating: vosk-model-small-en-us-0.15/ivector/final.mat  \n",
            "  inflating: vosk-model-small-en-us-0.15/README  \n"
          ]
        }
      ],
      "source": [
        "!wget https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip && unzip vosk-model-small-en-us-0.15.zip\n",
        "!mv vosk-model-small-en-us-0.15 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXG_5G87A2bD"
      },
      "source": [
        "After extracting the Vosk model, we can remove the downloaded zip file to free up disk space in our Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "otjw0pVI4LpA"
      },
      "outputs": [],
      "source": [
        "!rm vosk-model-small-en-us-0.15.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RlJQjxhHv-rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing Video <a name=\"processing-video\"></a>"
      ],
      "metadata": {
        "id": "vPCh-YizvBg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the YouTube Video <a name=\"downloading-video\"></a>"
      ],
      "metadata": {
        "id": "QwAHGAQSvPvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To begin, we'll fetch a video from YouTube. For this example, we'll use the [*How Google Search Works* video](https://www.youtube.com/watch?v=0eKVizvYSUQ), which describes how Google Search works, including how Google’s software indexes the web, ranks sites, flags spam, and serves up results.\n",
        "\n",
        "We'll use the `pytubefix` library to handle the download:"
      ],
      "metadata": {
        "id": "p-BLzpt5vhUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytubefix import YouTube\n",
        "\n",
        "yt_object = YouTube('https://www.youtube.com/watch?v=0eKVizvYSUQ')\n",
        "# print the title of the video\n",
        "print(f'Title: {yt_object.title}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ziCEAdFwDOT",
        "outputId": "e63a79d0-0ade-4c42-8774-4adfdbc49a29"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: How Google Search Works (in 5 minutes)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This following code retrieves the video, selects the highest resolution stream, and downloads it to the Colab environment."
      ],
      "metadata": {
        "id": "J-UdsVKXwzsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yt_object_name = 'how-google-search-works'\n",
        "# download the video stream\n",
        "yt_object_high_res = yt_object.streams.get_highest_resolution()\n",
        "print(f'Downloading: {yt_object_high_res}')\n",
        "yt_object_high_res.download(filename=f'{yt_object_name}.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "nj9xA5Z_wIhd",
        "outputId": "0aaf31e8-a43e-4cac-b6e4-291547294761"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: <Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/how-google-search-works.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBFOCvx_uY3j"
      },
      "source": [
        "### Extracting the Audio Track <a name=\"extracting-audio\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T68TIm0uv160"
      },
      "source": [
        "Now that have downloaded the video, let's extract the audio content using the `moviepy` library. The following code loads the video, extracts the audio track, and saves it as an audio file within the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAVI1qN_u5Vb",
        "outputId": "7c9fa09b-bdd1-4c83-cb1c-576cbd759c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in /content/how-google-search-works.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n"
          ]
        }
      ],
      "source": [
        "import moviepy.editor as mpe\n",
        "\n",
        "video = mpe.VideoFileClip(f'/content/{yt_object_name}.mp4')\n",
        "video.audio.write_audiofile(f'/content/{yt_object_name}.wav')\n",
        "video.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSAXVRXY13ib"
      },
      "source": [
        "### Transcribing the Audio <a name=\"transcribing-audio\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68OY-cQf1_LD"
      },
      "source": [
        "With the audio file ready, we'll transcribe it into text using the `SpeechRecognition` library and the Vosk API. The following code initializes a speech recognizer, loads the audio file, and performs speech-to-text transcription using Vosk."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import speech_recognition as sr\n",
        "\n",
        "# initialize the recognizer engine\n",
        "recognizer_engine = sr.Recognizer()\n",
        "\n",
        "# upload the audio file to be processed\n",
        "with sr.AudioFile(f'/content/{yt_object_name}.wav') as audio_file:\n",
        "  audio_track = recognizer_engine.record(audio_file)\n",
        "  audio_output = recognizer_engine.recognize_vosk(audio_track)"
      ],
      "metadata": {
        "id": "bj0_HX8P0y2O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** One limitation of the small model for English (i.e. `vosk-model-small-en-us-0.15`) is that it doesn't distinguish between different speakers."
      ],
      "metadata": {
        "id": "anDX9gHo2qvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the speech-to-text transcription has been performed, we can print the recognized text:"
      ],
      "metadata": {
        "id": "z4GPKzQ71W8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8gVR1-g6rZj",
        "outputId": "67f17bf5-bd35-473a-96e6-e26097205bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "everyday billions of people come here with questions about all kinds of things sometimes we even get questions about google search itself like how this whole thing actually works and while this is a subject entire books have been written about there's a good chance you're in the market for something a little more concise so let's say it's getting close to dinner and you want a recipe for lasagna you've probably seen this before the let's go a little deeper since the beginning back when the home page looked like this google has been continuously mapping the web hundreds of billions of pages to create something called an index think of it as the giant library we look through whenever you do a search for lasagna or anything else now the worthless on yeah shows up a lot on the web pages about the history of lasagna articles by scientists whose last name happened to be lasagna stuff other people might be looking for but if you're hungry randomly clicking through millions of links is no fun this is where googles ranking algorithms come into play first they try to understand what you're looking for so they can be helpful even if you don't know exactly the right word to use or if you're spelling as a little off then they sift through millions of possible matches and index and automatically assemble a page that tries to put the most relevant information up top for you to choose from okay now we have some results the how did the algorithms actually decide what made it onto the first page there are hundreds of factors that go into ranking search results so let's talk about a few of them you may already know that pages containing the words you search for are more likely to end up at the top no surprise there but the location of those words like in the pages title or in and images caption those are factors to there's a lot more to ranking than just words back when google got started we looked at how pages linked to each other to better understand what pages were about and how important and trustworthy they seemed today linking is still an important factor and other factors location where a search happens because if you happened to be in or mayor italy you might be looking for information about their annual is on a festival the but if you're in omaha nebraska you probably aren't when a web page was uploaded is an important factor two pages published more recently and often have more accurate information especially in the case of a rapidly developing new story of course not every site and the web is trying to be helpful just like with robocalls on your phone or spam and your email there are a lot of sites that only exists a scam and everyday scammers upload millions more of them so just because instant virus download dot net list the words lasagna recipe for hundred times that doesn't mean it's going to help you make dinner we spend a lot of time trying to stay one step ahead of tricks like these making sure our algorithms can recognize scam sites like them before they make it to your search results page so let's review billions of times a day whenever someone searches for lasagna or resume writing tips or how to swaddle a baby or anything else google software locates all the potentially relevant results in the web removes all the spam and ranks and based on hundreds of factors like keywords links location and freshness okay good time to take a breath this last part is about how we make changes to search and it's important since nineteen ninety eight when google one online people seem to have found our results pretty helpful but the web is always changing and people are always searching for new things in fact one in every seven searches is there something that's never been typed into the search box and for by anyone ever so we're always working and updates the search thousands every year which brings up a big question how do we decide whether change is making search more helpful well one of the ways we evaluate potential updates a search is by asking people like you every day thousands of search quality raiders look at samples of search results side by side then give feedback about the relevance and reliability of the information to make sure those evaluations are consistent the raiders follow a list of search quality evaluate our guidelines think of them as are publicly available guide what makes a good result good oh and one last thing to remember we use responses from raiders to evaluate changes but they don't directly impact how search results are ranked so there you have it every time you click search or algorithms are analyzing the meaning of the words in your search matching them to the content on the web understanding what content is most likely to be helpful unreliable and then automatically putting it all together in a neatly organized page design to get you the info you need all and how twenty one seconds wow anyone else ready for dinner interested in learning more we've got a whole website dedicated to how search works just click right here wanna read the search called a raider guidelines for yourself click right here\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# print the recognized text\n",
        "recognized_text = json.loads(audio_output)['text']\n",
        "print(recognized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarizing the Transcribed Text <a name=\"summarizing-text\"></a>"
      ],
      "metadata": {
        "id": "e-fG_mDz1xvG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize the transcribed text using the Gemini API, we will first need to set up the authentication. This relies on defining `GOOGLE_API_KEY` as a secret with your actual API key from [Google AI Studio](https://aistudio.google.com/app/apikey)."
      ],
      "metadata": {
        "id": "a_KbdpNi4d9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "kpKumr5K5XzG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A *system prompt* in generative AI is a set of instructions provided to a Large Language Model (LLM) before any user input, designed to guide the model's behavior and responses.\n",
        "\n",
        "To summarize the content of the video, we will use the following system prompt:"
      ],
      "metadata": {
        "id": "RqfXSmVx6BIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "# Objective\n",
        "You are an AI assistant specialized in marketing campaign. Your challenge is to write engaging content based on the transcript of a podcast.\n",
        "\n",
        "# Output\n",
        "The output should include the following:\n",
        "* Tagline: The tagline should encapsulate the essence of the topics discussed in the transcribed text.\n",
        "* Summary: Summary of text should be 500-1000 words that uses informative, concise and relevant language.\n",
        "\n",
        "# Compliance\n",
        "Write your response in Markdown.\n",
        "Do *not* include any information that is not found in the user input.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jaFamJpA6dWv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use the Gemini API to generate the summary:"
      ],
      "metadata": {
        "id": "aiXAt87O7H9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    model_name='models/gemini-1.5-flash',\n",
        "    system_instruction=system_prompt\n",
        ")\n",
        "response = model.generate_content(recognized_text)"
      ],
      "metadata": {
        "id": "V6tmPTSh74fg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's print the summary generated by the Gemini API:"
      ],
      "metadata": {
        "id": "lfehS38G8uR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tEHUt7t86OP",
        "outputId": "bcdc7529-ddf6-4323-908b-27b8ac8a4380"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Tagline: \n",
            "**Unveiling the Magic Behind Google Search: From Lasagna to the Latest News.**\n",
            "\n",
            "## Summary: \n",
            "Every day, billions of people use Google Search to find answers to all sorts of questions, even about search itself.  While there are entire books dedicated to explaining how Google works, this summary offers a concise overview. \n",
            "\n",
            "Imagine you're searching for a lasagna recipe. Google has a massive \"index,\" essentially a library of hundreds of billions of web pages. This index is constantly being updated, mapping the entire web to ensure it captures the latest information. \n",
            "\n",
            "However, simply throwing a vast amount of information at you isn't helpful.  That's where Google's ranking algorithms come into play.  These algorithms analyze your search query to understand what you're looking for, even if your wording is slightly off or your spelling isn't perfect.  Then, they sift through millions of potential matches within the index and prioritize the most relevant results at the top of the search results page.\n",
            "\n",
            "But how do these algorithms decide what makes it to the first page? There are hundreds of factors, but some of the most important include:\n",
            "\n",
            "* **Keywords:** Pages containing the words you search for are more likely to rank higher.  The location of these keywords, such as in the page title or image caption, also plays a role.\n",
            "* **Links:** Back when Google first started, it analyzed how pages linked to each other to determine their importance and trustworthiness.  While linking is still an important factor today, it's just one piece of the puzzle. \n",
            "* **Location:** If you're in Italy, you might be looking for information about their annual lasagna festival. If you're in Nebraska, you probably aren't.  Your location is a factor that can help Google tailor your results.\n",
            "* **Freshness:** Pages published more recently often have more accurate information, especially for rapidly developing news stories.\n",
            "* **Fighting Spam:** Not every website is designed to be helpful. Google continuously works to identify and remove spammy websites, ensuring that the results you see are reliable. \n",
            "\n",
            "To stay ahead of the curve, Google's algorithms are constantly being updated. In fact, one in seven searches is for something never searched for before.  Google evaluates these updates by asking people like you for feedback through search quality raters. These raters are trained to assess the relevance and reliability of search results, ensuring that the information you see is trustworthy and helpful. \n",
            "\n",
            "The next time you do a Google search, remember that a complex system is working behind the scenes, analyzing your query, sorting through millions of results, and presenting you with the most relevant and helpful information. All of this happens in a matter of seconds, providing you with the answers you need, whenever you need them. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion <a name=\"conclusion\"></a>"
      ],
      "metadata": {
        "id": "gbbZG1Wy4WoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explored a pipeline for processing MP4 videos, from audio extraction and transcription to text summarization using the Google Generative AI API.\n",
        "\n",
        "We leveraged libraries the following libraries\n",
        "* `pytubefix` to download a YouTube video\n",
        "* `moviepy` to extract the audio from the video\n",
        "* `speechrecognition` and `vosk` to transcribe the text from the audio\n",
        "* `google-generativeai` to summarize the content of the video\n",
        "\n",
        "While this demonstration offers a basic framework, there's ample room for customization and enhancement. Potential next steps include:\n",
        "* **Improving Transcription Accuracy**: Experiment with different Vosk models or explore cloud-based speech recognition services for potentially better accuracy.\n",
        "* **Fine-tuning Summarization**: Adjust parameters within the Gemini API call to tailor the summarization to your specific needs (e.g., length, focus).\n",
        "* **Adding Speaker Identification**: If distinguishing between speakers is important, investigate libraries or services that offer speaker diarization capabilities.\n",
        "\n",
        "By building upon this foundation, you can create powerful tools for extracting key insights from video content efficiently."
      ],
      "metadata": {
        "id": "Bud7kKGw4ctN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ZWXMCRyPq34r",
        "-WUqsBIarNMu",
        "msTxL71e4G0L",
        "QwAHGAQSvPvC",
        "XBFOCvx_uY3j"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPuJV9dmp0t+34e6uLho78j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}